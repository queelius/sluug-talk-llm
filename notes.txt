Watch previous SLUUG presenter's talk on LLMs / ChatGPT


## Why Local LLM?


privacy

control

lower latency potentially -- fastAPI end point that uses a tiny LLM like tinyllama? still need a lot of gpus if you want to scale up to thousands of users

experimentation

diy ethos

maybe cheaper?

explain:
	- safetensors: popular huggingface 

	- huggingface has half a million models of various kinds

	- gguf: binary format that allows the entire model to be contained in one single binary file
	- ollama: a nice local service (can be run as a background service) that can efficiently use any of your local ggufs
		- it's like llama.cpp, but in a nicer interface
		- you can load extra large ggufs that won't fit into graphics VRAM by either running it all on the cpu,
		  or partly on the cpu and partly on the GPU for a bit of a speedup.
		- i only have a nvidia 3600 w/12gb vram

	- let's show how to download a gguf. thebloke is very popular and has quantized all kinds of models that can be
	  run on small GPUs.

	- quantization: instead of using 16 bits to represent a floating point number (model weight), use 8, 4, even 2 bit.
		- sometimes quantizing weights can even improve accuracy, e.g., model may have been overfitted to training
		data and the quantization acts as a kind of regularizer



TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF
	


Gemini 1.5 (Google)
	- Link to paper. We can go over it. 1 million context and more.
	- Hints at 10 million tokens
		- Mention that one Haystack test
Sora (OpenAI)
	- Show videos
	- Link to technical report
	- Discuss ideas. World models? Intuitive physics engine? Show Jim Fan tweet.
	- Show Lecun. Show Google Deepmind RL twitter discussion.
		* Poke fun at self.

What is ChatGPT doing?
	- Link to Stephen Wolfram
	- Mention who he is.

n-grams
	- use my naive implementation to frame a lot of the discussion
	- in-context
	- generative / predictive
	


sampling procedures
	- I want to show the next-token probabilities in a nice UI. i think openai's playground does this.
	 * the cat sat on the _ <-- should show a bunch of possibilities
	 * also, color coding how probably the next-token is, red for unsure i think green for sure
	 * at inference time, increasing temperature should make everything more uniform (uncertain)
	 * at training time, it tries to learn the actual probability distribution of the data,
           so temperature is a rescaling setting for changing inference-time behavior
		- temp = 1 -- estimates prob dist (DGP).
		- temp < 1 -- reduces randomness, i.e., more predictable outputs
		- temp = 0 -- deterministic, the local argmax (given context, what is most
			probable next-token)
		- temp = infinity - uniform distribution on the top-k or top-p tokens
		- top-p = 0.9 means that, when you order tokens by most to least
		probable, you choose enough of the  most probable tokens such
		that they sum to 0.9 prob. these tokens will be sampled from,
		temperature applies only to them.
		- top-k = 10 means include the 10 most probable tokens in the possible outputs 
		temperature applies only to them.
	- talk about other sampling methods
		- talk about what sampling methods are doing at inference time.
		- mention how inference time sampling is almost always OOD, since
		when we train it, we don't train it on its own outputs normally, but the raw data.
		* beam search -- explore multiple paths and sample the most likely? not sure.
		* prompting strategies: CoT, ToT, CoT-SC, etc etc 



talk about tokens:

	- linguistic tokens
		- word2vec, semantically rich representations, more context and richness the representation the deeper the layer of the NN
	- multi-modality
		- tokens of all kinds of sorts, from audio tokens to image tokens to, with Sora, space-time tokens (think of stacking video frames)


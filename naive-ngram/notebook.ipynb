{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://colab.research.google.com/drive/1ak4kOtbIQGXE5kuhhGTd55xu4qRpeZd7?usp=sharing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Model (AR) Over Bytes\n",
    "\n",
    "We consider an AR language model over bytes (256 tokens).\n",
    "\n",
    "- We give it algorithmic training data.\n",
    "- We can then see how well the model can predict the next token given the context\n",
    "- Since we know the data generating process, we can see how well the model captures the underlying process.\n",
    "    - *Spoiler*: It doesn't do well.\n",
    "\n",
    "- This is a Markov chain on the order of $O(256^n)$ states at maximum, but the\n",
    "algortihmic data is low-dimensional so it's very *sparse*.\n",
    "\n",
    "- We represent our $n$-gram model as a dictionary of dictionaries.\n",
    "    - Outer dictionary is indexed by the context.\n",
    "    - Inner dictionary is indexed by the next token.\n",
    "    - Each token given the context maps to the number of times that token was\n",
    "    observed in the training data.\n",
    "        - Normalize by the total count to get a probability.\n",
    "\n",
    "- This is simple model and simple data\n",
    "    - Hopefully, exploring its properties can help us understand LLMs.\n",
    "\n",
    "### Finite State Machines\n",
    "\n",
    "We can view AR-LMs as finite state machines (if deterministic) otherwise\n",
    "Markvo chains without loss of generality.\n",
    "\n",
    "- Computers are FSMs, just very large ones.\n",
    "- LLMs are also very large FSMs.\n",
    "\n",
    "https://www.lesswrong.com/posts/7qSHKYRnqyrumEfbt\n",
    "\n",
    "- Thus, AR-LLMs are differentiable computers that can learn from examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "\n",
    "class MarkovChain:\n",
    "    def __init__(self):\n",
    "        self.model = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        pprint(self.model)\n",
    "\n",
    "    def percept(self, prev_tokens, next_token):\n",
    "        if prev_tokens not in self.model:\n",
    "            self.model[prev_tokens] = {}\n",
    "        if next_token not in self.model[prev_tokens]:\n",
    "            self.model[prev_tokens][next_token] = 0\n",
    "        self.model[prev_tokens][next_token] += 1\n",
    "\n",
    "    def train(self, data, order = 100):\n",
    "        N = len(data)\n",
    "        for i in range(N):\n",
    "            tokens = data[i]\n",
    "            m = len(tokens)\n",
    "            if i % 10000 == 0:\n",
    "                print(f'{i/N*100} percent complete')\n",
    "            for j in range(m):\n",
    "                for k in range(0, order+1):\n",
    "                    if m <= j+k:\n",
    "                        break\n",
    "                    self.percept(prev_tokens = tokens[j:j+k],\n",
    "                                 next_token = tokens[j+k])\n",
    "\n",
    "    def distribution(self, ctx):\n",
    "        if ctx not in self.model:\n",
    "            return self.distribution(ctx[1:])        \n",
    "        total = sum(self.model[ctx].values())\n",
    "        return self.model[ctx] | {k: v / total for k, v in self.model[ctx].items()}\n",
    "    \n",
    "    def predict(self, ctx):\n",
    "        d = self.distribution(ctx)\n",
    "        r = random.random()\n",
    "        s = 0\n",
    "        for token, p in d.items():\n",
    "            s += p\n",
    "            if s >= r:\n",
    "                return token\n",
    "        raise ValueError('no candidate found')\n",
    "\n",
    "    def generate(self, ctx, max_tokens = 100, stop_token='.'):\n",
    "        output = ''\n",
    "        for i in range(max_tokens):\n",
    "            token = self.predict(ctx)\n",
    "            output += token\n",
    "            if token == stop_token:\n",
    "                break\n",
    "            ctx = ctx + token\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Training Data\n",
    "\n",
    "Our training data are byte sequences of expression trees.\n",
    "- The language has a few basic operations: `srt` (sort), `sum`, `min`, and `max`.\n",
    "- Each operation operations on lists of integers and returns lists of integers.\n",
    "- We train the model on *random* expression trees.\n",
    "- We can then use the model to **generate** new expression trees\n",
    "- We can **prompt** it with a partial expression trees to see how well it predicts the rest of the tree.\n",
    "\n",
    "\n",
    "![expression-tree](./expr_tree.png)\n",
    "\n",
    "Here is a depiction of `srt[sum[4,3],max[3,5,2]]=[6,7]`.\n",
    "    - If we prompt the model with `srt[sum[4,3],max[3,5,2]]=`, we expect it to predict `[6,7]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def srt(x):\n",
    "    return sorted(x)\n",
    "\n",
    "def default_args():\n",
    "    return {\n",
    "        'operations': [sum, sum, min, max, srt],\n",
    "        'recurse_prob': 0.25,\n",
    "        'min_child': 1,\n",
    "        'max_child': 5,\n",
    "        'values': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        'min_depth': 0,\n",
    "        'max_depth': 4,\n",
    "        'debug': False\n",
    "    }\n",
    "\n",
    "def generate_data(\n",
    "        samples=1, \n",
    "        stop_tok='.',\n",
    "        args=default_args()):\n",
    "\n",
    "    for k in default_args().keys():\n",
    "        if k not in args:\n",
    "            args[k] = default_args()[k]\n",
    "\n",
    "    sample = []\n",
    "    for _ in range(samples):\n",
    "        data = generate_tree(\n",
    "            args['operations'],\n",
    "            args['recurse_prob'],\n",
    "            args['min_child'],\n",
    "            args['max_child'],\n",
    "            args['values'],\n",
    "            args['min_depth'],\n",
    "            args['max_depth'],\n",
    "            args['debug'])\n",
    "\n",
    "        result = data['result']\n",
    "        if isinstance(result, list) and len(result) == 1:\n",
    "            result = result[0]\n",
    "\n",
    "        tok_seq = f\"{data['expr']}={result}{stop_tok}\"\n",
    "        tok_seq.replace(' ', '')\n",
    "        sample.append(tok_seq)\n",
    "    return sample\n",
    "\n",
    "def generate_tree(operations, recurse_prob, min_child, max_child,\n",
    "        values, min_depth, max_depth, debug, offset=''):\n",
    "    \n",
    "    if max_depth != 0 and (min_depth >= 0 or random.random() < recurse_prob):\n",
    "        num_nodes = random.randint(min_child, max_child)\n",
    "\n",
    "        childs = [generate_tree(\n",
    "            operations=operations,\n",
    "            recurse_prob=recurse_prob,\n",
    "            min_child=min_child,\n",
    "            max_child=max_child,\n",
    "            values=values,\n",
    "            min_depth=min_depth-1,\n",
    "            max_depth=max_depth-1,\n",
    "            debug=debug,\n",
    "            offset = offset + '  ') for _ in range(num_nodes)]\n",
    "        \n",
    "        child_results = []\n",
    "        for child in childs:\n",
    "            if not isinstance(child['result'], list):\n",
    "                child['result'] = [child['result']]\n",
    "            child_results.extend(child['result'])\n",
    "        op = random.choice(operations)   \n",
    "        res = op(child_results)\n",
    "        if not isinstance(res, list):\n",
    "            res = [res]\n",
    "        expr = f'{op.__name__}[{\",\".join([child[\"expr\"] for child in childs])}]'\n",
    "        data = {'result': res, 'expr': expr}\n",
    "        if debug:\n",
    "            print(f'{offset}{data=}')\n",
    "        return data\n",
    "    else:\n",
    "        v = random.choice(values)\n",
    "        data = {'result': v, 'expr': f'{v}'}\n",
    "        \n",
    "        if debug:\n",
    "            print(f'{offset}{data=}')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate some data. It's going to be the simplest data. It's \n",
    "an expression tree 1 level deep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6=6.', '2=2.', '5=5.', '7=7.', '8=8.', '1=1.', '6=6.', '7=7.', '7=7.', '4=4.']\n"
     ]
    }
   ],
   "source": [
    "sample = generate_data(\n",
    "   samples=10,\n",
    "   args={'debug': False, 'min_depth': 0, 'max_depth': 0})\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a bit more complicated data, `operation[list]=list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['min[6,5,2,2]=2.', 'min[9,8,8,2,1]=1.', 'sum[3,5,7,9]=24.']\n"
     ]
    }
   ],
   "source": [
    "sample = generate_data(\n",
    "   samples=1000,\n",
    "   args={'debug': False, 'min_depth': 1, 'max_depth': 1})\n",
    "print(sample[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it! This is pretty straight forward to learn, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent complete\n"
     ]
    }
   ],
   "source": [
    "model = MarkovChain()\n",
    "model.train(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7,3,8,8,8]=34.\n",
      "]=1.\n",
      "3.\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('sum'))\n",
    "print(model.generate('sum[1'))\n",
    "print(model.generate('sum[1,2,3]='))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just learned the most inefficient way to evaluate a simple expression\n",
    "tree. Let's extend the number of operands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]=10.\n",
      "]=14.\n",
      "]=[2, 3, 4, 5].\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('sum[1,2,3,4'))\n",
    "print(model.generate('sum[1,2,3,4,5'))\n",
    "print(model.generate('sum[1,2,3,4,5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sum[1,2,3,4,5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum[1,2,3,4,5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sum[1,2,3,4,5'"
     ]
    }
   ],
   "source": [
    "model.model['sum[1,2,3,4,5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never seen this data before. So, what does it do?\n",
    "It throws away the oldest bytes until it has seen the content.\n",
    "Then, it predicts the next byte based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{']': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "']=14.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.model['2,3,4,5'])\n",
    "model.generate('2,3,4,5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inductive Bias\n",
    "\n",
    "Throwing away oldest bytes is a strong inductive bias.\n",
    "\n",
    "- Not necessarily true that the next byte is dependent on the oldest bytes.\n",
    "\n",
    "## Generative Model\n",
    "- Generate text by starting with a any context and then sampling from the\n",
    "probability distribution for that context to get the next token.\n",
    "- Repeat until we have generated the desired number of tokens.\n",
    "- Same way LLMs work (but they work well).\n",
    "\n",
    "## Analysis\n",
    "Our model has some advantages compared to transformer-based AR-LLMs:\n",
    "\n",
    "- Since we simply *store* the data:\n",
    "    - Easy to implement.\n",
    "    - Easy to make it a lifelong learner. Store *more data*.\n",
    "\n",
    "But, compared to more sophisticated models, they have huge disadvantages:\n",
    "\n",
    "- $n$-gram model is not able to capture long-range dependencies in the data.\n",
    "    - Number of states grows exponentially with the order of the model.\n",
    "    - It cannot scale to large contexts, and therefore cannot understand\n",
    "    nuances in the data.\n",
    "\n",
    "- $n$-gram model does not generalize out-of-distribution very well.\n",
    "    - Since language is a high-dimensional space, *most* contexts have never been seen before.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "A *good* model *compresses* the data.\n",
    "\n",
    "- This is a key concept in ML.\n",
    "- There is a notion that *compression* is a proxy for *understanding*.\n",
    "- Take a *physics simulation*: we don't need to store the position and velocity\n",
    "of every particle.\n",
    "- We can just store the starting conditions and then let the laws of physics\n",
    "play out.\n",
    "    - Not perfect, but perfectly predicting the future is not possible.\n",
    "    - We only need to predict it well enough to make good decisions.\n",
    "\n",
    "`Prediction = compression = intelligence`\n",
    "\n",
    "The brain may be a good example of this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

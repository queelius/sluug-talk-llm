{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Model (AR) Over Bytes\n",
    "\n",
    "We consider an AR language model over bytes (256 tokens). When we give it\n",
    "algorithmic training data, we are free to speicfy the max contet length, or\n",
    "the order of the model, we want to consider. We can then see how well the model\n",
    "can predict the next token given the context, and how well it can generate\n",
    "new data.\n",
    "\n",
    "This is a Markov chain on the order of $O(256^n)$ states at maximum, but the\n",
    "data is low-dimensional so we can train the model with a relatively high order\n",
    "$n$.\n",
    "\n",
    "We represent our n-gram model as a dictionary of dictionaries, where the outer\n",
    "dictionary is indexed by the context and the inner dictionary is indexed by the\n",
    "next token and contains the number of times that token was observed in that\n",
    "context in the training data. (Normalize by the total count to get a probability).\n",
    "\n",
    "This is a dead simple model, but here we use it to explore the properties of\n",
    "language models and as a way to understand LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "\n",
    "class MarkovChain:\n",
    "    def __init__(self):\n",
    "        self.model = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        pprint(self.model)\n",
    "\n",
    "    def percept(self, prev_tokens, next_token):\n",
    "        if prev_tokens not in self.model:\n",
    "            self.model[prev_tokens] = {}\n",
    "        if next_token not in self.model[prev_tokens]:\n",
    "            self.model[prev_tokens][next_token] = 0\n",
    "        self.model[prev_tokens][next_token] += 1\n",
    "\n",
    "    def train(self, data, order = 100):\n",
    "        N = len(data)\n",
    "        for i in range(N):\n",
    "            tokens = data[i]\n",
    "            m = len(tokens)\n",
    "            if i % 10000 == 0:\n",
    "                print(f'{i/N*100} percent complete')\n",
    "            for j in range(m):\n",
    "                for k in range(0, order+1):\n",
    "                    if m <= j+k:\n",
    "                        break\n",
    "                    self.percept(prev_tokens = tokens[j:j+k],\n",
    "                                 next_token = tokens[j+k])\n",
    "\n",
    "    def distribution(self, ctx):\n",
    "        if ctx not in self.model:\n",
    "            return self.distribution(ctx[1:])        \n",
    "        total = sum(self.model[ctx].values())\n",
    "        return self.model[ctx] | {k: v / total for k, v in self.model[ctx].items()}\n",
    "    \n",
    "    def predict(self, ctx):\n",
    "        d = self.distribution(ctx)\n",
    "        r = random.random()\n",
    "        s = 0\n",
    "        for token, p in d.items():\n",
    "            s += p\n",
    "            if s >= r:\n",
    "                return token\n",
    "        raise ValueError('no candidate found')\n",
    "\n",
    "    def generate(self, ctx, max_tokens = 100, stop_token='.'):\n",
    "        output = ''\n",
    "        for i in range(max_tokens):\n",
    "            token = self.predict(ctx)\n",
    "            output += token\n",
    "            if token == stop_token:\n",
    "                break\n",
    "            ctx = ctx + token\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithmic Training Data\n",
    "\n",
    "Our training data are byte sequences of expression trees in a simple language\n",
    "that we define. The language has a few basic operations, such as `srt` (sort),\n",
    "`sum`, `min`, and `max`. Each of these operatiosn takes a list of integers as\n",
    "input and returns a list of integers as output. We generate random expression\n",
    "trees in this language, and then use them to train the model. We can then use\n",
    "the model to generate new expression trees, and if we prompt it with a partial\n",
    "expression tree, we can see how well it can predict the rest of the tree.\n",
    "\n",
    "Here is a depiction of `srt[sum[4,3],max[3,5,2]]=[6,7]`. If we prompt the model\n",
    "with `srt[sum[4,3],max[3,5,2]]=`, we expect it to predict `[6,7]`.\n",
    "\n",
    "![expression-tree](./expr_tree.png)\n",
    "\n",
    "Feel free to play around with `generate_data`. It's in the file\n",
    "`algorithmic_data.py`. We reproduce it in the code below.\n",
    "You can change the operations, the number of operations, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def srt(x):\n",
    "    return sorted(x)\n",
    "\n",
    "def default_args():\n",
    "    return {\n",
    "        'operations': [sum, sum, min, max, srt],\n",
    "        'recurse_prob': 0.25,\n",
    "        'min_child': 1,\n",
    "        'max_child': 5,\n",
    "        'values': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        'min_depth': 0,\n",
    "        'max_depth': 4,\n",
    "        'debug': False\n",
    "    }\n",
    "\n",
    "def generate_data(\n",
    "        samples=1, \n",
    "        stop_tok='.',\n",
    "        args=default_args()):\n",
    "\n",
    "    for k in default_args().keys():\n",
    "        if k not in args:\n",
    "            args[k] = default_args()[k]\n",
    "\n",
    "    sample = []\n",
    "    for _ in range(samples):\n",
    "        data = generate_tree(\n",
    "            args['operations'],\n",
    "            args['recurse_prob'],\n",
    "            args['min_child'],\n",
    "            args['max_child'],\n",
    "            args['values'],\n",
    "            args['min_depth'],\n",
    "            args['max_depth'],\n",
    "            args['debug'])\n",
    "\n",
    "        result = data['result']\n",
    "        if isinstance(result, list) and len(result) == 1:\n",
    "            result = result[0]\n",
    "\n",
    "        tok_seq = f\"{data['expr']}={result}{stop_tok}\"\n",
    "        tok_seq.replace(' ', '')\n",
    "        sample.append(tok_seq)\n",
    "    return sample\n",
    "\n",
    "def generate_tree(operations, recurse_prob, min_child, max_child,\n",
    "        values, min_depth, max_depth, debug, offset=''):\n",
    "    \n",
    "    if max_depth != 0 and (min_depth >= 0 or random.random() < recurse_prob):\n",
    "        num_nodes = random.randint(min_child, max_child)\n",
    "\n",
    "        childs = [generate_tree(\n",
    "            operations=operations,\n",
    "            recurse_prob=recurse_prob,\n",
    "            min_child=min_child,\n",
    "            max_child=max_child,\n",
    "            values=values,\n",
    "            min_depth=min_depth-1,\n",
    "            max_depth=max_depth-1,\n",
    "            debug=debug,\n",
    "            offset = offset + '  ') for _ in range(num_nodes)]\n",
    "        \n",
    "        child_results = []\n",
    "        for child in childs:\n",
    "            if not isinstance(child['result'], list):\n",
    "                child['result'] = [child['result']]\n",
    "            child_results.extend(child['result'])\n",
    "        op = random.choice(operations)   \n",
    "        res = op(child_results)\n",
    "        if not isinstance(res, list):\n",
    "            res = [res]\n",
    "        expr = f'{op.__name__}[{\",\".join([child[\"expr\"] for child in childs])}]'\n",
    "        data = {'result': res, 'expr': expr}\n",
    "        if debug:\n",
    "            print(f'{offset}{data=}')\n",
    "        return data\n",
    "    else:\n",
    "        v = random.choice(values)\n",
    "        data = {'result': v, 'expr': f'{v}'}\n",
    "        \n",
    "        if debug:\n",
    "            print(f'{offset}{data=}')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate some data. It's going to be the simplest data. It's \n",
    "an expression tree 1 level deep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9=9.', '1=1.', '1=1.', '4=4.', '4=4.', '8=8.', '4=4.', '5=5.', '3=3.', '8=8.']\n"
     ]
    }
   ],
   "source": [
    "sample = generate_data(\n",
    "   samples=10,\n",
    "   args={'debug': False, 'min_depth': 0, 'max_depth': 0})\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a bit more complicated data, `operation[list]=list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['min[2,3,6]=2.', 'srt[4,9,4,6]=[4, 4, 6, 9].', 'min[3,7,1]=1.']\n"
     ]
    }
   ],
   "source": [
    "sample = generate_data(\n",
    "   samples=100000,\n",
    "   args={'debug': False, 'min_depth': 1, 'max_depth': 1})\n",
    "print(sample[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it! This is pretty straight forward to learn, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 percent complete\n",
      "10.0 percent complete\n",
      "20.0 percent complete\n",
      "30.0 percent complete\n",
      "40.0 percent complete\n",
      "50.0 percent complete\n",
      "60.0 percent complete\n",
      "70.0 percent complete\n",
      "80.0 percent complete\n",
      "90.0 percent complete\n"
     ]
    }
   ],
   "source": [
    "model = MarkovChain()\n",
    "model.train(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8,6]=14.\n",
      ",8,6]=15.\n",
      "6.\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('sum'))\n",
    "print(model.generate('sum[1'))\n",
    "print(model.generate('sum[1,2,3]='))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just learned the most inefficient way to evaluate a simple expression\n",
    "tree. Let's extend the number of operands!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]=10.\n",
      "]=14.\n",
      "]=[2, 3, 4, 5].\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('sum[1,2,3,4'))\n",
    "print(model.generate('sum[1,2,3,4,5'))\n",
    "print(model.generate('sum[1,2,3,4,5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sum[1,2,3,4,5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum[1,2,3,4,5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sum[1,2,3,4,5'"
     ]
    }
   ],
   "source": [
    "model.model['sum[1,2,3,4,5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's never seen this data before. So, what does it do?\n",
    "It throws away the oldest bytes until it has seen the content.\n",
    "Then, it predicts the next byte based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{']': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "']=14.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.model['2,3,4,5'])\n",
    "model.generate('2,3,4,5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This throwing away of the oldest bytes is a strong inductive bias. It's not\n",
    "necessarily true that the next byte is dependent on the oldest bytes. It's\n",
    "just a simple way to handle unseen data. It's a simple way to handle the\n",
    "fact that most states are out-of-distribution, particularly in high-dimensional\n",
    "spaces -- unlike this contrived example for algorithmic data.\n",
    "\n",
    "We can then generate text by starting with a any context and then sampling from\n",
    "the probability distribution for that context to get the next token.\n",
    "\n",
    "Compared to more sophisticated models, like transformer-based models, it\n",
    "performs poorly. Here's why:\n",
    "\n",
    "1. The $n$-gram model is not able to capture long-range dependencies in the data\n",
    "as well, given that the number of states grows exponentially with the order\n",
    "of the model.\n",
    "\n",
    "2. The $n$-gram model does not generalize out-of-distribution very well.\n",
    "Since language is a high-dimensional space, *most* contexts have never been\n",
    "seen before.\n",
    "\n",
    "The $n$-gram model does not in practice capture the semantics of a natural\n",
    "language very well. It is sample inefficient and does not scale to large\n",
    "contexts.\n",
    "\n",
    "In our model, we simply *store* the data. This has advantages and disadvantages:\n",
    "\n",
    "Advantages:\n",
    "- It's simple and easy to implement.\n",
    "- It's easy to make it a lifelong learner, because we can simply add new data\n",
    "  to the model. (This is currently a problem for LLMs, which are not lifelong.)\n",
    "Disadvantages:\n",
    "- It's not sample efficient. It requires a lot of data to learn the model.\n",
    "- It's not scalable. It requires a lot of memory to store the model.\n",
    "- It doesn't generalize well OOD. A lot of tricks have been tried to improve\n",
    "  it, but compared to LLMs, they suck.\n",
    "\n",
    "A *good* model *compresses* the data. This is a key concept in machine learning.\n",
    "There is a notion that *compression* is a proxy for *understanding*.\n",
    "Take a physics simulation, for example. We don't need to store the position\n",
    "and velocity of every particle in the universe. We can just store the\n",
    "starting conditions and then let the laws of physics play out. It won't be\n",
    "perfect, but perfectly predicting the future is not possible -- we only need\n",
    "to predict it well enough to make good decisions.\n",
    "\n",
    "`Prediction = compression = intelligence`\n",
    "\n",
    "The brain may be a good example of this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
